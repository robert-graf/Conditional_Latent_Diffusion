model:
  base_learning_rate: 0.000001
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.0015
    linear_end: 0.0195
    num_timesteps_cond: 1
    log_every_t: 400
    timesteps: 1000
    first_stage_key: image
    cond_stage_key: c_concat # Produced by the dataset
    image_size: 32 #After downscale (A)
    channels: 4 #After downscale (B without concat; is used to creat the random noise for sampling)
    cond_stage_trainable: false # We do not want the encoder to be trained.
    conditioning_key: concat # hybrid,crossattn,concat
    monitor: val/loss
    use_ema: True    
    
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 8 #(A) Dependen on first stage
        in_channels: 8 #(B+concat) Dependen on first stage
        out_channels: 4 #(B)
        model_channels: 64 #192
        attention_resolutions: [ 1, 2, 4 ]
        num_res_blocks: 2
        channel_mult: [ 1,2,2,4]  # 32, 16, 8, 4, 2
        num_heads: 8
        use_spatial_transformer: false
        #transformer_depth: 1
        #context_dim: 512
        use_scale_shift_norm: True
        resblock_updown: True

    
    first_stage_config:
      config_path: logs/2024-09-25T12-05-27_autoencoder_kl_32x32x4_gray/autoencoder_kl_32x32x4_gray.yaml
      ckpt_path: logs/2024-09-25T12-05-27_autoencoder_kl_32x32x4_gray/checkpoints/last.ckpt
    
    cond_stage_config:
      config_path: logs/2024-09-25T12-05-27_autoencoder_kl_32x32x4_gray/autoencoder_kl_32x32x4_gray.yaml
      ckpt_path: logs/2024-09-25T12-05-27_autoencoder_kl_32x32x4_gray/checkpoints/last.ckpt

data:
  target: main.DataModuleFromConfig
  
  params:
    batch_size: 64
    wrap: True
    num_workers: 16 # Depends on the maschien
    train:
      target: ldm.data.T2w_2_CT.ImageSRTrain
      params:
        size: 256
    validation:
      target: ldm.data.T2w_2_CT.ImageSRValidation
      params:
        size: 256

trainer:
  max_epochs: 10000
